"""
Session State Model

Represents the complete state of a game session at any point in time.
State is derived from events and can be reconstructed by replaying the event log.

Key Principles:
- State is immutable (copy-on-write pattern)
- Chat history is NOT the source of truth
- Context summaries are generated by the engine
- State can be serialized to JSON for snapshots
"""

from typing import Optional, List, Any
from pydantic import BaseModel, Field, ConfigDict


class StepScore(BaseModel):
    """
    Score information for a single step.
    Tracks both raw score and pass/fail status.
    """
    step_index: int
    score: int
    max_possible: int
    passed: bool
    attempts: int = 1


class DisplayMessage(BaseModel):
    """
    Message for display in the UI.
    These are what the user sees, not the source of truth.
    """
    role: str  # "gm", "user", "system"
    content: str
    timestamp: str  # ISO 8601 format
    metadata: Optional[dict] = None  # Optional scoring/hint info


class SessionState(BaseModel):
    """
    Complete state of a game session.
    This is what gets replayed from events and stored in snapshots.
    """
    model_config = ConfigDict(validate_assignment=True, arbitrary_types_allowed=True)

    # Session identification
    session_id: str
    challenge_id: str
    user_id: str

    # Current position
    current_step_index: int = 0
    status: str = "active"  # active, completed, failed, abandoned

    # Scoring state (engine owns this, never LLM)
    step_scores: List[StepScore] = Field(default_factory=list)
    total_score: int = 0
    max_possible_score: int = 100

    # Display state (what UI shows)
    messages: List[DisplayMessage] = Field(default_factory=list)
    current_ui_mode: str = ""  # CHAT, MCQ_SINGLE, MCQ_MULTI, etc.
    current_ui_data: Optional[dict] = None  # MCQ options, etc.

    # Tracking metrics
    mistakes_count: int = 0
    hints_used: int = 0

    # Context for LLMs (engine-generated, max 500 tokens)
    context_summary: str = ""

    # Flags for conditional logic (e.g., "showed_hint_on_step_1": true)
    flags: dict[str, Any] = Field(default_factory=dict)

    def calculate_final_percentage(self) -> float:
        """Calculate final completion percentage."""
        if self.max_possible_score == 0:
            return 0.0
        return (self.total_score / self.max_possible_score) * 100

    def is_step_passed(self, step_index: int) -> bool:
        """Check if a specific step was passed."""
        for step_score in self.step_scores:
            if step_score.step_index == step_index:
                return step_score.passed
        return False

    def get_step_score(self, step_index: int) -> Optional[StepScore]:
        """Get score for a specific step."""
        for step_score in self.step_scores:
            if step_score.step_index == step_index:
                return step_score
        return None

    def add_message(self, role: str, content: str, timestamp: str, metadata: Optional[dict] = None):
        """Add a message to the display history."""
        self.messages.append(DisplayMessage(
            role=role,
            content=content,
            timestamp=timestamp,
            metadata=metadata
        ))

    def update_context_summary(self, summary: str):
        """
        Update the context summary for LLM calls.
        Engine should keep this concise (max 500 tokens).
        """
        self.context_summary = summary[:2000]  # Rough character limit for ~500 tokens
